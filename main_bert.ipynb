{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jhsualv/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from seaborn import heatmap\n",
    "import seaborn as sns\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(documents, max_features):\n",
    "    \"\"\"\n",
    "    Computes the bag-of-words matrix for the given documents.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: List of text documents.\n",
    "    - max_features: Maximum number of features (vocabulary size).\n",
    "\n",
    "    Returns:\n",
    "    - bow_matrix: Sparse matrix of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    return bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(documents, max_features):\n",
    "    \"\"\"\n",
    "    Computes the tfidf matrix for the given documents.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: List of text documents.\n",
    "    - max_features: Maximum number of features to use.\n",
    "\n",
    "    Returns:\n",
    "    - tfidf_matrix: Sparse matrix of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "   \n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()  # Extract feature names\n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lsa(tfidf_matrix, n_components):\n",
    "    \"\"\"\n",
    "    Applies LSA (using TruncatedSVD) to the tfidf matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - tfidf_matrix: Sparse matrix from tfidf vectorization.\n",
    "    - n_components: Number of components to keep.\n",
    "\n",
    "    Returns:\n",
    "    - lsa_matrix: Dense matrix with reduced dimensions.\n",
    "    \"\"\"\n",
    " \n",
    "    vectorizer = TruncatedSVD(n_components=n_components)\n",
    "    lsa_matrix = vectorizer.fit_transform(tfidf_matrix)\n",
    "    return lsa_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(df):\n",
    "  documents = df['review'].tolist()\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "  labels = df['voted_up'].tolist()\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vader_sentiment_scores(documents):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = []\n",
    "\n",
    "    for doc in documents:\n",
    "        sentiment = analyzer.polarity_scores(doc)\n",
    "        scores.append(sentiment)  # This is a dict with 'neg', 'neu', 'pos', 'compound'\n",
    "\n",
    "    return pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "documents = create_documents(df)\n",
    "\n",
    "#vader_scores_df = compute_vader_sentiment_scores(documents) #create the scores (and for now do literally nothing with them)\n",
    "#df = pd.concat([df, vader_scores_df], axis=1)\n",
    "\n",
    "labels = create_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# version check on imports \n",
    "import transformers\n",
    "import accelerate\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ebc797e09c4aef8bac022f4662e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702665d5fe82407db7b9b0100d8afdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jhsualv/Documents/SteamReviewAnalysis/steam_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 16:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhsualv/Documents/SteamReviewAnalysis/steam_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jhsualv/Documents/SteamReviewAnalysis/steam_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jhsualv/Documents/SteamReviewAnalysis/steam_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jhsualv/Documents/SteamReviewAnalysis/steam_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.078125, 'eval_f1': 0.0, 'eval_runtime': 3.2628, 'eval_samples_per_second': 19.615, 'eval_steps_per_second': 0.306, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "# Prepare documents and labels into a huggingface dataset\n",
    "df_for_bert = pd.DataFrame({'text': documents, 'label': labels})\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_for_bert['text'].tolist(), df_for_bert['label'].tolist(), \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove raw text after tokenization\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "val_dataset = val_dataset.remove_columns([\"text\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_linear_regression(tfidf_matrix, labels, test_size=0.2, random_state=42, plot_confusion_matrix=False):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model to predict review votes and evaluates it.\n",
    "\n",
    "    Parameters:\n",
    "    - tfidf_matrix: Sparse matrix of features (from TF-IDF).\n",
    "    - labels: Binary labels (0 = not voted up, 1 = voted up).\n",
    "    - test_size: Proportion of the dataset to include in the test split.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    - plot_confusion_matrix: Whether to plot the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained Linear Regression model.\n",
    "    - mse: Mean Squared Error on the test set.\n",
    "    - accuracy: Accuracy on the test set after thresholding at 0.5.\n",
    "    - confusion_mat: Confusion matrix.\n",
    "    - class_report: Text summary of precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        tfidf_matrix, labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Threshold predictions at 0.5 for classification\n",
    "    y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    \n",
    "    # Compute Confusion Matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred_class)\n",
    "    \n",
    "    # Compute Classification Report\n",
    "    class_report = classification_report(y_test, y_pred_class)\n",
    "    \n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Optionally plot confusion matrix\n",
    "    if plot_confusion_matrix:\n",
    "        sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model, mse, accuracy, confusion_mat, class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, mse, acc, cm, report = compute_linear_regression(tfidf_matrix, labels, plot_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive_bayes(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset, trains a Naive Bayes classifier (MultinomialNB),\n",
    "    and evaluates performance.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix (may not allow LSA features because negative values).\n",
    "    - y: Ground truth labels.\n",
    "    - test_size: Fraction of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Accuracy score on the test set.\n",
    "    - nb_model: The trained Naive Bayes model.\n",
    "    \"\"\"\n",
    "    # 1. Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    nb_model = MultinomialNB()\n",
    "    #nb_model = GaussianNB()        # uncomment for lsa tfidf\n",
    "\n",
    "    # 3. Train\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # 4. Predict\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "\n",
    "    # 5. Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Naive Bayes Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return accuracy, nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run everything\n",
    "\n",
    "y = create_labels(df)\n",
    "\n",
    "bow_matrix = compute_bow(documents, max_features)\n",
    "X_tfidf, feature_names = compute_tfidf(documents, max_features)\n",
    "X_lsa = apply_lsa(X_tfidf, 100)\n",
    "vader_scores = compute_vader_sentiment_scores(documents) #create the scores\n",
    "\n",
    "X_tfidf = pd.DataFrame(X_tfidf.toarray())\n",
    "bow_matrix = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "vader_scores_shifted = vader_scores.copy()\n",
    "for col in ['pos', 'neu', 'neg', 'compound']:\n",
    "    vader_scores_shifted[col] += 1\n",
    "\n",
    "# Make sparse\n",
    "vader_scores_sparse = vader_scores_shifted.astype(pd.SparseDtype(\"float\", fill_value=0))\n",
    "\n",
    "#vader_scores = vader_scores.astype(pd.SparseDtype(\"float\", fill_value=0))\n",
    "X_tfidf_vader = pd.concat([X_tfidf, vader_scores_sparse], axis=1)\n",
    "X_tfidf_vader.columns = X_tfidf_vader.columns.astype(str)\n",
    "#X_tfidf_vader = hstack([X_tfidf, vader_scores.sparse.to_coo()])\n",
    "bow_matrix_vader = pd.concat([bow_matrix, vader_scores_sparse], axis=1)\n",
    "bow_matrix_vader.columns = bow_matrix_vader.columns.astype(str)\n",
    "#bow_matrix_vader = hstack([X_tfidf, vader_scores.sparse.to_coo()])\n",
    "\n",
    "#NB for tf-idf\n",
    "print(\"TF-IDF NB:\")\n",
    "acc_nb, model_nb = evaluate_naive_bayes(X_tfidf, y)\n",
    "\n",
    "print(\"TF-IDF NB with VADER:\")\n",
    "acc_nb, model_nb = evaluate_naive_bayes(X_tfidf_vader, y)\n",
    "\n",
    "# NB for BoW\n",
    "print(\"BoW NB: \")\n",
    "acc_nb_bow, model_nb_bow = evaluate_naive_bayes(bow_matrix, y)\n",
    "\n",
    "print(\"BoW NB with VADER: \")\n",
    "acc_nb_bow, model_nb_bow = evaluate_naive_bayes(bow_matrix_vader, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Get the log probabilities of features for each class\n",
    "# Assuming model_nb is a trained MultinomialNB or similar Naive Bayes model\n",
    "class_log_probs = model_nb.feature_log_prob_\n",
    "\n",
    "# Focus on one class (e.g., class 1)\n",
    "class_index = 1  # Change to 0 or 1 depending on the class you want to visualize\n",
    "feature_importances = np.exp(class_log_probs[class_index])  # Convert log probabilities to probabilities\n",
    "\n",
    "# Create a dictionary mapping words to their importance\n",
    "word_importance = {feature_names[i]: feature_importances[i] for i in range(len(feature_names))}\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_word_importance = {\n",
    "    word: importance\n",
    "    for word, importance in word_importance.items()\n",
    "    if word not in ENGLISH_STOP_WORDS\n",
    "}\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(filtered_word_importance)\n",
    "\n",
    "\n",
    "# title based on class index\n",
    "\n",
    "title = \"Word Cloud of Important Features for Positive Reviews\" if class_index == 1 else \"Word Cloud of Important Features for Negative Reviews\"\n",
    "\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the adjective importance dictionary to a DataFrame\n",
    "adjective_df = pd.DataFrame(list(filtered_word_importance.items()), columns=['Word', 'Importance'])\n",
    "\n",
    "# Sort by importance and select the top N features\n",
    "top_n = 20\n",
    "top_adjectives = adjective_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_adjectives['Word'], top_adjectives['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the highest importance at the top\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Words')\n",
    "plt.title(f'Top {top_n} Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('all')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Get the log probabilities of features for each class\n",
    "# Assuming model_nb is a trained MultinomialNB or similar Naive Bayes model\n",
    "class_log_probs = model_nb.feature_log_prob_\n",
    "\n",
    "# Focus on one class (e.g., class 1)\n",
    "\n",
    "feature_importances = np.exp(class_log_probs[class_index])  # Convert log probabilities to probabilities\n",
    "\n",
    "# Create a dictionary mapping words to their importance\n",
    "word_importance = {feature_names[i]: feature_importances[i] for i in range(len(feature_names))}\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_word_importance = {\n",
    "    word: importance\n",
    "    for word, importance in word_importance.items()\n",
    "    if word not in ENGLISH_STOP_WORDS\n",
    "}\n",
    "\n",
    "# Filter to include only adjectives\n",
    "def is_adjective(word):\n",
    "    pos = pos_tag([word])[0][1]  # Get the part of speech tag\n",
    "    return pos.startswith('JJ')  # Adjectives in NLTK are tagged as 'JJ', 'JJR', or 'JJS'\n",
    "\n",
    "adjective_word_importance = {\n",
    "    word: importance\n",
    "    for word, importance in filtered_word_importance.items()\n",
    "    if is_adjective(word)\n",
    "}\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(adjective_word_importance)\n",
    "\n",
    "\n",
    "# title based on class index\n",
    "title = \"Word Cloud of Important Features for Positive Reviews\" if class_index == 1 else \"Word Cloud of Important Features for Negative Reviews\"\n",
    "\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the adjective importance dictionary to a DataFrame\n",
    "adjective_df = pd.DataFrame(list(adjective_word_importance.items()), columns=['Word', 'Importance'])\n",
    "\n",
    "# Sort by importance and select the top N features\n",
    "top_n = 20\n",
    "top_adjectives = adjective_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_adjectives['Word'], top_adjectives['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the highest importance at the top\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Adjectives')\n",
    "plt.title(f'Top {top_n} Adjectives')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"cleaned_data.csv\"  # Update this with the correct path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the length of each review\n",
    "df['review_length'] = df['review'].astype(str).apply(len)\n",
    "\n",
    "# Group by 'voted_up' and calculate the average review length\n",
    "avg_review_length = df.groupby('voted_up')['review_length'].median()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "avg_review_length.plot(kind='bar', color=['skyblue', 'orange'], edgecolor='black')\n",
    "\n",
    "# Update the x-axis labels\n",
    "plt.title('Median Review Length by Vote Status')\n",
    "plt.xlabel('Vote Status')\n",
    "plt.ylabel('Average Review Length')\n",
    "plt.xticks(ticks=[0, 1], labels=['Voted Down', 'Voted Up'], rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the length of each review\n",
    "df['review_length'] = df['review'].astype(str).apply(len)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    df[df['voted_up'] == 0]['review_length'],  # Review lengths for 'Not Voted Up'\n",
    "    [0] * len(df[df['voted_up'] == 0]),       # Corresponding y-values (voted_up = 0)\n",
    "    color='#FF9999', alpha=0.6, label='Voted Down'\n",
    ")\n",
    "plt.scatter(\n",
    "    df[df['voted_up'] == 1]['review_length'],  # Review lengths for 'Voted Up'\n",
    "    [1] * len(df[df['voted_up'] == 1]),       # Corresponding y-values (voted_up = 1)\n",
    "    color='#66B3FF', alpha=0.6, label='Voted Up'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Scatter Plot of Review Length vs. Vote Status', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Review Length', fontsize=12)\n",
    "plt.ylabel('Voted Up Status', fontsize=12)\n",
    "plt.yticks([0, 1], ['Voted Down', 'Voted Up'])  # Custom y-axis labels\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE Visualizations (2D, 3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=130, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(lsa_matrix)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Create the scatter plot\n",
    "scatter2D = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='coolwarm', alpha=0.7, s=10)\n",
    "\n",
    "plt.colorbar(scatter2D, label='Recommended (1) / Not Recommended (0)')\n",
    "plt.title('t-SNE Visualization of Steam Review Embeddings')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t-SNE with 3 components, fit using LSA-reduced matrix\n",
    "tsne_3d = TSNE(n_components=3, random_state=130, perplexity=30)\n",
    "X_tsne_3d = tsne_3d.fit_transform(lsa_matrix)\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create the scatter plot\n",
    "scatter3D = ax.scatter(X_tsne_3d[:, 0], X_tsne_3d[:, 1], X_tsne_3d[:, 2], c=labels, cmap='coolwarm', alpha=0.7, s=15)\n",
    "\n",
    "ax.set_title(\"3D t-SNE Visualization of Steam Reviews\")\n",
    "fig.colorbar(scatter3D, label='Recommended (1) / Not Recommended (0)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-N Feature Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the log probabilities of features\n",
    "class_log_probs = model_nb.feature_log_prob_\n",
    "\n",
    "# Get the difference between class 1 and class 0 (Recommended vs Not Recommended)\n",
    "log_prob_diff = class_log_probs[1] - class_log_probs[0]  # Higher = more likely to be in recommended reviews\n",
    "\n",
    "# Map words to importance difference and filter out common/junk words\n",
    "word_importance = {feature_names[i]: log_prob_diff[i] for i in range(len(feature_names))}\n",
    "\n",
    "custom_stopwords = ENGLISH_STOP_WORDS.union({'game', 'games', 'player', 'players'})\n",
    "filtered_word_importance = {word: importance for word, importance in word_importance.items() if word.lower() not in custom_stopwords and word.isalpha() and len(word) > 2}\n",
    "\n",
    "# Create DataFrame of word importances\n",
    "importance_df = pd.DataFrame(list(filtered_word_importance.items()), columns=['Word', 'Importance'])\n",
    "\n",
    "# Get the Top N positive (recommended) and Top N negative (not recommended) words\n",
    "top_n = 20\n",
    "top_positive = importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "top_negative = importance_df.sort_values(by='Importance', ascending=True).head(top_n)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_positive['Word'], top_positive['Importance'], color='green', label='Recommended')\n",
    "plt.barh(top_negative['Word'], top_negative['Importance'], color='red', label='Not Recommended')\n",
    "plt.xlabel('Log-Probability Difference')\n",
    "plt.title('Top Informative Words for Sentiment')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Top-N Feature Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from BoW vectorizer\n",
    "bow_vectorizer = CountVectorizer(max_features=max_features)\n",
    "bow_vectorizer.fit(documents)\n",
    "bow_feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get feature importance (difference between class log probs)\n",
    "class_log_probs_bow = model_nb_bow.feature_log_prob_\n",
    "log_prob_diff_bow = class_log_probs_bow[1] - class_log_probs_bow[0]\n",
    "\n",
    "# Build dataframe\n",
    "word_importance_bow = {bow_feature_names[i]: log_prob_diff_bow[i] for i in range(len(bow_feature_names))}\n",
    "\n",
    "# Filter out common stopwords\n",
    "filtered_word_importance_bow = {word: importance for word, importance in word_importance_bow.items() if word.lower() not in custom_stopwords and word.isalpha() and len(word) > 2}\n",
    "\n",
    "importance_df_bow = pd.DataFrame(\n",
    "    list(filtered_word_importance_bow.items()), columns=['Word', 'Importance']\n",
    ")\n",
    "\n",
    "# Plot Top-N Features\n",
    "top_n = 20\n",
    "top_positive_bow = importance_df_bow.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "top_negative_bow = importance_df_bow.sort_values(by='Importance', ascending=True).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_negative_bow['Word'], top_negative_bow['Importance'], color='red', label='Not Recommended')\n",
    "plt.barh(top_positive_bow['Word'], top_positive_bow['Importance'], color='green', label='Recommended')\n",
    "plt.axvline(0, color='black')\n",
    "plt.xlabel('Log-Probability Difference')\n",
    "plt.title(f'Top {top_n} Most Informative Words (BoW)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log probs of the features for each class\n",
    "class_log_probs_bow = model_nb_bow.feature_log_prob_\n",
    "\n",
    "# Focus on a single class\n",
    "class_index = 1  # 1 = recommended, 0 = not recommended\n",
    "feature_importances = np.exp(class_log_probs[class_index])  # convert log-probs to probs\n",
    "\n",
    "# Create word importance dictionary\n",
    "word_importance = {bow_feature_names[i]: feature_importances[i] for i in range(len(bow_feature_names))}\n",
    "\n",
    "# Filter out common stopwords + junk\n",
    "filtered_word_importance = {word: importance for word, importance in word_importance.items() if word.lower() not in custom_stopwords and word.isalpha() and len(word) > 2}\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(filtered_word_importance)\n",
    "\n",
    "# Plot\n",
    "title = \"Word Cloud of Important Features for Positive Reviews (BoW)\" if class_index == 1 else \"Word Cloud of Important Features for Negative Reviews\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(title)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
